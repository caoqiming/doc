**贝尔曼方程 (Bellman Equation)**

贝尔曼方程是动态规划和强化学习的灵魂。简单来说，它把一个复杂的决策问题分解成：**“现在的奖励” + “未来的价值”**。
它是以数学家 Richard Bellman 的名字命名的，核心思想是：**最优策略具有这样的性质——无论初始状态和决策如何，对于前一次决策所形成的状态而言，余下的决策必须构成对于该状态的最优策略。**

## 1. 核心直觉：拆解价值

想象你在迷宫中，贝尔曼方程告诉你，某个格子 的**价值**等于：

1. 你走出这一步能立刻拿到的**奖励 (Reward)**。
2. 加上你到达下一个格子 后，从那里开始一直到终点的**预期价值 (Discounted Future Value)**。

## 2. 数学表达式

在 MDP（马尔可夫决策过程）中，最常见的贝尔曼方程形式如下：

$$
V(s) = \max_{a} \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V(s')]
$$

### 变量拆解：

- $V(s)$：当前状态 的价值
- $\max_a$：我们要选择那个能让总收益最大的动作 $a$
- $P(s'|s, a)$：转移概率，即在状态 $s$ 下采取动作 $a$ 后，有多大可能跳到状态 $s'$
- $R(s, a, s')$：即时奖励
- $\gamma$ ：折扣因子（通常在 0 到 1 之间）。它代表了你对“未来钱”的重视程度： 越小你越近视， 越大你越有远见
- $V(s')$：下一个状态的价值

$s'$ 的取值范围是由动作 $a$ 决定的（一个动作会有概率到达若干的不同的新状态）， $V(s)$ 是所有动作当中，平均收益最高的动作的平均收益。

## 3. 为什么它如此重要？

在贝尔曼方程出现之前，如果你想计算一个策略的好坏，你需要模拟成千上万条从头到尾的路径，计算量巨大。

**贝尔曼方程提供了一种“递归”的解法：**

- 它建立了**相邻状态**之间的联系。
- 它让计算机可以通过**迭代**的方式，像“水波扩散”一样，通过局部的不断更新，最终计算出整个地图的价值分布。
